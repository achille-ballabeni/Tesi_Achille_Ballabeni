{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load original training dataset to be split in partitions."
      ],
      "metadata": {
        "id": "X5pJzjPJ_Wwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset in the TFrecord format."
      ],
      "metadata": {
        "id": "JbycAVVUsr8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1wD3vKqKEFh6OfrfLNtOENF-lbe4auQDb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kAxgw_AsvBs",
        "outputId": "8cb0032a-febb-45a0-8a25-c15acf81b6af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wD3vKqKEFh6OfrfLNtOENF-lbe4auQDb\n",
            "To: /content/train_tfrecords0.record\n",
            "100% 2.94G/2.94G [00:21<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a set of loading and decoding functions."
      ],
      "metadata": {
        "id": "0UT6Gymh8-MP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGZowrcFbBas"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def load_tf_records(filepath):\n",
        "    filenames = tf.io.gfile.glob(filepath)\n",
        "    dataset = tf.data.TFRecordDataset(filenames,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def tf_records_file_features_description():\n",
        "    image_feature_description = {\n",
        "        \n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image': tf.io.FixedLenFeature([],tf.string),\n",
        "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label/P': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'label/K': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'label/Mg': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'label/Ph': tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "    return image_feature_description\n",
        "\n",
        "def decode_dataset(example_proto):\n",
        "    features=tf.io.parse_single_example(example_proto, tf_records_file_features_description())\n",
        "\n",
        "    image=features['image']\n",
        "    height=features['image/height']\n",
        "    width=features['image/width']\n",
        "    image=tf.io.decode_raw(image,tf.int16)\n",
        "    image=tf.reshape(image,[height,width,150])\n",
        "    filename=features['image/filename']\n",
        "\n",
        "    P=features['label/P']\n",
        "    K=features['label/K']\n",
        "    Mg=features['label/Mg']\n",
        "    Ph=features['label/Ph']\n",
        "\n",
        "    height=features['image/height']\n",
        "    width=features['image/width']\n",
        "\n",
        "    label=[P,K,Mg,Ph]\n",
        "\n",
        "    return image, label, height, width"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load training dataset."
      ],
      "metadata": {
        "id": "2hhWkApf9JY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s0GBVKRzbHxN"
      },
      "outputs": [],
      "source": [
        "# Training dataset filepath\n",
        "dataset_tf_records_path = '/content/train_tfrecords0.record'\n",
        "\n",
        "dataset = load_tf_records(dataset_tf_records_path).map(decode_dataset, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle and split training dataset into 5 partitions."
      ],
      "metadata": {
        "id": "PNvweUXsGtbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 1732\n",
        "\n",
        "dataset = dataset.shuffle(1732, seed=958).cache()\n",
        "for i in dataset:  # iterate over dataset so that it is cached and the new resulting sets do not have overlapping elements\n",
        "    pass\n",
        "\n",
        "split_1 = dataset.take(346)\n",
        "split_2 = dataset.skip(346).take(346)\n",
        "split_3 = dataset.skip(692).take(346)\n",
        "split_4 = dataset.skip(1038).take(347)\n",
        "split_5 = dataset.skip(1385)\n",
        "\n",
        "splits = [split_1, split_2, split_3, split_4, split_5]"
      ],
      "metadata": {
        "id": "rHZpQwWiGssy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save newly created partitions as TFRecord files."
      ],
      "metadata": {
        "id": "LHxgrWlhAhEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a set of funtions required to save the new partitions as TFRecord files."
      ],
      "metadata": {
        "id": "ojQA4qBE-Eee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define some utilities\n",
        "\n",
        "def int64_feature(value):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "\n",
        "def int64_list_feature(value):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "\n",
        "def bytes_feature(value):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def bytes_list_feature(value):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
        "\n",
        "\n",
        "def float_feature(value):\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "def float_list_feature(value):\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
      ],
      "metadata": {
        "id": "_AsVJqoE1Ye0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the encoding of the resulting tfrecords file\n",
        "\n",
        "def create_tf_example(image, label, height, width):\n",
        "\n",
        "    image=image.numpy().tobytes()\n",
        "\n",
        "    P=label[0]\n",
        "    K=label[1]\n",
        "    Mg=label[2]\n",
        "    Ph=label[3]\n",
        "    \n",
        "    #This is needed for Object detection API and shall be coherent with the label map\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': int64_feature(height),\n",
        "        'image/width': int64_feature(width),\n",
        "        'image': bytes_feature(image),\n",
        "\n",
        "        'label/P': float_feature(P),\n",
        "        'label/K': float_feature(K),\n",
        "        'label/Mg': float_feature(Mg),\n",
        "        'label/Ph': float_feature(Ph),\n",
        "        \n",
        "          \n",
        "    }))\n",
        "    return tf_example"
      ],
      "metadata": {
        "id": "5A4CyCTS1YcP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tfrecord_file_from_ds(dataset, output_path):\n",
        "\n",
        "    #Define the desired number of TFRecords files, for TPU parallel data loading 100MB is the optimal file size\n",
        "    number_of_tfrecords_files=1\n",
        "\n",
        "    images_processed=0\n",
        "\n",
        "    for i in range(number_of_tfrecords_files):\n",
        "        writer = tf.io.TFRecordWriter(output_path)\n",
        "    \n",
        "    \n",
        "        for image, label, height, width in dataset:\n",
        "            images_processed+=1\n",
        "            features=label\n",
        "      \n",
        "            tf_example = create_tf_example(image, label, height, width)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "\n",
        "        writer.close()\n",
        "        print('Successfully created the TFRecord file: {}'.format(output_path))"
      ],
      "metadata": {
        "id": "kYpKucnl1YXA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save TFRecord files to hosted runtime path."
      ],
      "metadata": {
        "id": "feVQrBF6-ikO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output path\n",
        "output_path='/content/split_{}.record'\n",
        "\n",
        "i=0\n",
        "for data in splits:\n",
        "    i+=1\n",
        "    save_tfrecord_file_from_ds(data, output_path.format(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QC4qU8iuTTo",
        "outputId": "5d3cc141-9b81-45cb-c74d-f01ef1139e47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: /content/split_1.record\n",
            "Successfully created the TFRecord file: /content/split_2.record\n",
            "Successfully created the TFRecord file: /content/split_3.record\n",
            "Successfully created the TFRecord file: /content/split_4.record\n",
            "Successfully created the TFRecord file: /content/split_5.record\n"
          ]
        }
      ]
    }
  ]
}
